---
title: "Assignment13"
author: "Yifei"
date: "2018/11/28"
output: html_document
---

1. Title: Glass Identification Database

2. Sources:
    (a) Creator: B. German
        -- Central Research Establishment
           Home Office Forensic Science Service
           Aldermaston, Reading, Berkshire RG7 4PN
    (b) Donor: Vina Spiehler, Ph.D., DABFT
               Diagnostic Products Corporation
               (213) 776-0180 (ext 3014)
    (c) Date: September, 1987

3. Past Usage:
    -- Rule Induction in Forensic Science
       -- Ian W. Evett and Ernest J. Spiehler
       -- Central Research Establishment
          Home Office Forensic Science Service
          Aldermaston, Reading, Berkshire RG7 4PN
       -- Unknown technical note number (sorry, not listed here)
       -- General Results: nearest neighbor held its own with respect to the
             rule-based system

4. Relevant Information:n
      Vina conducted a comparison test of her rule-based system, BEAGLE, the
      nearest-neighbor algorithm, and discriminant analysis.  BEAGLE is 
      a product available through VRS Consulting, Inc.; 4676 Admiralty Way,
      Suite 206; Marina Del Ray, CA 90292 (213) 827-7890 and FAX: -3189.
      In determining whether the glass was a type of "float" glass or not,
      the following results were obtained (# incorrect answers):

             Type of Sample                            Beagle   NN    DA
             Windows that were float processed (87)     10      12    21
             Windows that were not:            (76)     19      16    22

      The study of classification of types of glass was motivated by 
      criminological investigation.  At the scene of the crime, the glass left
      can be used as evidence...if it is correctly identified!

5. Number of Instances: 214

6. Number of Attributes: 10 (including an Id#) plus the class attribute
   -- all attributes are continuously valued

7. Attribute Information:
   1. Id number: 1 to 214
   2. RI: refractive index
   3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as 
                  are attributes 4-10)
   4. Mg: Magnesium
   5. Al: Aluminum
   6. Si: Silicon
   7. K: Potassium
   8. Ca: Calcium
   9. Ba: Barium
  10. Fe: Iron
  11. Type of glass: (class attribute)
      -- 1 building_windows_float_processed
      -- 2 building_windows_non_float_processed
      -- 3 vehicle_windows_float_processed
      -- 4 vehicle_windows_non_float_processed (none in this database)
      -- 5 containers
      -- 6 tableware
      -- 7 headlamps

8. Missing Attribute Values: None

Summary Statistics:
Attribute:   Min     Max      Mean     SD      Correlation with class
 2. RI:       1.5112  1.5339   1.5184  0.0030  -0.1642
 3. Na:      10.73   17.38    13.4079  0.8166   0.5030
 4. Mg:       0       4.49     2.6845  1.4424  -0.7447
 5. Al:       0.29    3.5      1.4449  0.4993   0.5988
 6. Si:      69.81   75.41    72.6509  0.7745   0.1515
 7. K:        0       6.21     0.4971  0.6522  -0.0100
 8. Ca:       5.43   16.19     8.9570  1.4232   0.0007
 9. Ba:       0       3.15     0.1750  0.4972   0.5751
10. Fe:       0       0.51     0.0570  0.0974  -0.1879

9. Class Distribution: (out of 214 total instances)
    -- 163 Window glass (building windows and vehicle windows)
       -- 87 float processed  
          -- 70 building windows
          -- 17 vehicle windows
       -- 76 non-float processed
          -- 76 building windows
          -- 0 vehicle windows
    -- 51 Non-window glass
       -- 13 containers
       -- 9 tableware
       -- 29 headlamps

```{r}
#1
glasstb <- read.csv("glass.csv", col.names = c("num","Rl","Na","Mg","Al","Si","K","Ca","Ba","Fe","TypeOfClass"), header = FALSE)
```

```{r}
#2 
#there is no missing data such like NA, and all data are in good shape in either int or double, and seems like normal distributed.
```


```{r}
#3 Create a histogram of the Na column and overlay a normal curve; visually determine whether the data is normally distributed. You may use the code from this tutorial. Does the k-NN algorithm require normally distributed data or is it a non-parametric method? Comment on your findings. 

n <- glasstb$Na
h <- hist(n)
xfit <- seq(min(n), max(n),length = 40) 
yfit <- dnorm(xfit, mean = mean(n), sd = sd(n)) 
yfit <- yfit * diff(h$mids[1:2]) * length(n) 
lines(xfit, yfit, col = "black", lwd = 2)

#it is normally distributied.
```

```{r}
#4 Identify outliers; describe your identification approach and how many and which columns were removed, and why.


outlier_count <- function(i){
  sd <- sd(i)
  mean <- mean(i)
  lower <- mean-3*sd
  upper <- mean+3*sd
  k <- i[(i<lower) | (i>upper)]
  return(length(k))
}
   
lapply(glasstb, outlier_count)
#this produces how many outliers in each

```
```{r}
# remove the outlier
library(dplyr)
outlier_remove <- function(table){
   for(i in 1:ncol(table)){
      col1_sd <- sd(table[,i])
  col1_mean <- mean(table[,i])
    output<- table %>%
      filter(col1_mean-3*col1_sd<table[,i]&table[,i]< col1_mean+3*col1_sd)
}
  return(output)
}


```
```{r}
#5 After removing the ID column (column 1), normalize the first two columns in the data set using min-max normalization.
#remove ID column
glasstb <- glasstb[,-1]

#normalize the first two column using min-max normalization


normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

Rl_Na_n <- as.data.frame(lapply(glasstb[1:2], normalize))

summary(Rl_Na_n)
```

```{r}
#6 Normalize the remaining columns, except the last one, using z-score standardization. The last column is the glass type and so it is excluded.

znormalize <- function(x) {
  return ((x - mean(x)) / sd(x))
}

others.n <- as.data.frame(lapply(glasstb[3:9], znormalize))
summary(others.n)

```

```{r}
n_total <- cbind.data.frame(Rl_Na_n, others.n)
n_total <- cbind.data.frame(n_total, glasstb[10])

```


```{r}
#7 The data set is sorted, so creating a validation data set requires random selection of elements. Create a stratified sample where you randomly select 50% of each of the cases for each glass type to be part of the validation data set. The remaining cases will form the training data set.

rownum <- nrow(glasstb)
validationsetindex <- sample(1:rownum, rownum*0.5, replace = FALSE)
validation_set <- n_total[validationsetindex,]
train_set <-n_total[-validationsetindex,]
train_set_label <- n_total[-validationsetindex, 10]
validation_set_label <- n_total[validationsetindex, 10]

```

```{r}
#8 Implement the k-NN algorithm in R (do not use an implementation of k-NN from a package) and use your algorithm with a k=10 to predict the glass type for the following two cases:
# RI = 1.51621 | 12.53 | 3.48 | 1.39 | 73.39 | 0.60 | 8.55 | 0.00 | Fe = 0.05
# RI = 1.5098 | 12.77 | 1.85 | 1.81 | 72.69 | 0.59 | 10.01 | 0.00 | Fe = 0.01
#calculating distance first
calculateDist <- function(a,b){
  d = 0
  for(i in c(1:(length(a)-1) )){
    d = d + (a[[i]]-b[[i]])^2
  }
  d = sqrt(d)
  return(d)
}

#prediction function
knn_predict <- function(test, train, k_val){
  pred1 <- rep(0, nrow(test))
  #loop the test data
  for(i in c(1:nrow(test))){
    #initial vector variable with empty, and good bad variable with 0 value
  e_dis = c()
  e_int = c()
  
  for(j in c(1:nrow(train))){
    #add distance between train set and test set
    e_dis <- c(e_dis, calculateDist(test[i,],train[j,]))
    e_int <- c(e_int, as.integer(train[j,][[10]]))
  }
  #e is a data frame contain the type and the distance
  #we put them in order select the first k smallest distance data
  e <- data.frame(e_int, e_dis)
  e <- e[order(e$e_dis),]
  e <- e[1: k_val,]

  #show the answer
 e_table <- table(e[1])  
  answer <- names(which.max(e_table))
  #paste them into answer storage
  pred1[i] <- answer
  }
  
  return(pred1)
  
}

#first I need to  fill the data with a number, no matter that is, because when we predict we need the dimension to be the same

pr <- knn_predict(validation_set, train_set,10)
pr
```


```{r}
#predicting the two values
exp1 <- data.frame(1.51621,12.53,3.48,1.39,73.39,0.6,8.55,0.00,0.05,0)
exp2 <- data.frame(1.5098,12.77,1.85,1.81,72.69,0.59,10.01,0.00,0.01,0)
exp1_pr <- knn_predict(exp1,train_set,10)
exp2_pr <- knn_predict(exp2,train_set,10)
exp1_pr
#in my case, predict the first example is type 2, sometimes 7
exp2_pr
#in my case,predict the second example is type 2, sometimes 7

```

```{r}
#9 Apply the knn function from the class package with k=11 and redo the cases from Question (8).
library(class)
pred <- knn(train = train_set, test = validation_set ,cl = train_set_label, k =11)

predict_exp2 <- knn(train = train_set,test = exp2, cl= train_set_label, k=11)
predict_exp1 <- knn(train = train_set,test = exp1, cl= train_set_label, k=11)
predict_exp2
#the type we predict is 2, in my case, sometimes 7
predict_exp1
#the type we predict is 2, sometimes 7
```
```{r}
#10 Determine the accuracy of the knn function with k=11 from the class package by applying it against each case in the validation data set. What is the percentage of correct classifications?
table(validation_set[,"TypeOfClass"],pred)
```
```{r}
total<- nrow(validation_set)
#this is my case, u might see different one after i knit to html
answer <- (31+37+6+0+0+15)/total
answer
# The percentage of correct classification is 83.17%
```

```{r}
#11 Determine an optimal k by trying all values from 5 through 11 for your own k-NN algorithm implementation against the cases in the validation data set. What is the optimal k, i.e., the k that results in the best accuracy? Plot k versus accuracy.
k5 <- knn_predict(validation_set,train_set,5)
k6 <- knn_predict(validation_set,train_set,6)
k7 <- knn_predict(validation_set,train_set,7)
k8 <- knn_predict(validation_set,train_set,8)
k9 <- knn_predict(validation_set,train_set,9)
k10 <- knn_predict(validation_set,train_set,10)
k11 <- knn_predict(validation_set,train_set,11)
library(MLmetrics)
a5<- Accuracy(validation_set[,"TypeOfClass"], k5)
a6<-Accuracy(validation_set[,"TypeOfClass"], k6)
a7<-Accuracy(validation_set[,"TypeOfClass"], k7)
a8<-Accuracy(validation_set[,"TypeOfClass"], k8)
a9<-Accuracy(validation_set[,"TypeOfClass"], k9)
a10<-Accuracy(validation_set[,"TypeOfClass"], k10)
a11<-Accuracy(validation_set[,"TypeOfClass"], k11)

accuracy_v <- c(a5,a6,a7,a8,a9,a10,a11)
plot(accuracy_v)
#where 5 is the highest in my previous case, but u can see each case from the graph where 1 is a5, 7 is a11
```

```{r}
#12 Create a plot of k (x-axis) versus error rate (percentage of incorrect classifications).
er5 <- 1-a5
er6 <- 1-a6
er7 <- 1-a7
er8 <- 1-a8
er9 <- 1-a9
er10 <- 1-a10
er11 <- 1-a11

error.rate <-c(er5,er6,er7,er8,er9,er10,er11)

plot(error.rate)

#when k value is 11, the error rate is highest in my case, which could be different in the html u see, but you can always look at the graph, the highest point has the highest error rate.

```

```{r}
#13  Produce a cross-table confusion matrix showing the accuracy of the classification using a package of your choice and a k of your choice.
#I choose the matrix when k is equal to 10
pred10 <- knn(train = train_set, test = validation_set ,cl = train_set_label, k =10)
pred10
```

```{r}
ConfusionMatrix(validation_set[,"TypeOfClass"],pred10)
```

```{r}
#14 Comment on the run-time complexity of the k-NN for classifying w new cases using a training data set of n cases having m features. Assume that m is "large". How does this algorithm behave as w, n, and m increase? Would this algorithm be "fast" if the training data set and the number of features are large?
#the algorithm in run time is O(nm+ wm). and grow exponentially,
# The way I am thinking is that first we have to run n cases of training set, which is O(n), for each case, we run m features, so its O(n*m), and for the new cases we want to run the features on them as well, so it is O(w*m), however, we might also want to consider a fix constant value k, because we run each thing k times, if we dont ignore that we just time all the cases by k, however if we ignore that the run-time is O(nm+wm).
```











