---
title: "Assignment12"
author: "Yifei"
date: "2018/11/26"
output: html_document
---

```{r}  

#1.Divide the provided data set into random two subsets: a training data set (70%)  and a test data set (30%).
titanic_data <- read.csv("titanic_data.csv")
titanic_data$Survived <- as.factor(titanic_data$Survived)
titanic_data$Parch<- as.factor(titanic_data$Parch)
titanic_data$Sex <- as.factor(titanic_data$Sex)
titanic_data$Embarked <- as.factor(titanic_data$Embarked)
titanic_data$Pclass <- as.factor(titanic_data$Pclass)
total_numhber_passengerId <- nrow(titanic_data)

#get trainset
titanic_data$Age[which(is.na(titanic_data$Age))] <- mean(na.omit(titanic_data$Age))
trainingsetindex <- sample(1:total_numhber_passengerId, total_numhber_passengerId*0.7, replace = FALSE)
trainingset <- titanic_data[trainingsetindex,]

#get testset
testsetindex <- sample(1:total_numhber_passengerId, total_numhber_passengerId*0.3, replace = FALSE)
testset <-titanic_data[testsetindex,]

```

```{r}
#2.Construct a logistic regression model to predict the probability of a passenger surviving the Titanic accident.
trainingset$Survived <- as.factor(trainingset$Survived)

#build model
survivemodel <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = trainingset, family = "binomial")

survivemodel

```

```{r}
#3.Test the statistical significance of all parameters and eliminate those that have a p-value > 0.05

summary(survivemodel) #this shows the p-value that is greater than 0.05
```

```{r}
#4 The parameter on left side has p value larger than 0.05 are:
# Parch, Fare, Cabin, Embarked
# Then we rebuild the model


survivemodel_update <- glm(Survived ~ Pclass + Sex + Age + SibSp, data = trainingset, family = "binomial")


survivemodel_update
```

```{r}
#4 Test the model against the test data set and determine its prediction accuracy (as a percentage correct)

survived <- predict.glm(survivemodel_update, testset, type = "response")


survived[survived>0.5 ] = 1
survived[!survived>0.5] = 0

original <- testset$Survived

library(MLmetrics)
Accuracy(survived, original)
# the accuracy is around 80% when I am doing it
```

```{r}
#5. Determine if the model has bias, e.g., false positives (someone is called dead when they actually survived) vs false negatives (someone classified as survived when they actually died).

ConfusionMatrix(survived, testset$Survived)

#false positive: y_true is 1 when it actually survive, 0 when actually dead, 
#false negative: y_pred is 1 when we predict it survive, 0 when predict it dead.
# By looking at the matrix, we observe that false positives (someone is called dead when they actually survived) has appeared about 30 times, false negatives (someone classified as survived when they actually died) has appeared about 30 times. The numbers are very similar, so there is no bias.

#the answer is the time when I am doing it, random number might different the output. But most of the time it doesnt have bias

```

